{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1T1zbLYtqM2f19qB0jsEY8_TTdSaAXYOg","timestamp":1770497091012}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","# DL Assignment 01\n","\n","**Name:**\n","\n","**Course Email:**  \n","\n","This is a small assignment that connects topics from Module 1, 2, and 3.  \n","\n","## End of Assignment\n","\n","Before submitting:\n","- Run all cells from top to bottom.  \n","- Check that all answer sections are filled.  \n","- Instruction video অনুযায়ী আমাদের দেয়া Colab ফাইলটি থেকে প্রথম একটি Save copy in drive করে নিবা। এরপর Google colab এর মধ্যে কোডগুলো করবে এবং সেই ফাইলটি ‘Anyone with the link’ & ‘View’ Access দিয়ে ফাইলটির Shareble Link টি সাবমিট করবে।"],"metadata":{"id":"JcVoA-oxVACn"}},{"cell_type":"markdown","source":["# Question 01: [ Marks 10 ]\n","\n","What is a perceptron?\n","Explain its three main components and their roles."],"metadata":{"id":"d18z86IcVO3B"}},{"cell_type":"markdown","source":["**Write** Answer 01:\n","\n","A perceptron is a basic artificial neuron used for binary classification in machine learning.\n","\n","Its three main components are:\n","\n","1. Inputs: These are the feature values given to the perceptron that represent the data.\n","\n","2. Weights: Each input has an associated weight that determines how important that input is in making the decision.\n","\n","3. Activation function: This takes the weighted sum of inputs and decides the final output, traditionally using a step (threshold) function.\n"],"metadata":{"id":"uEW6ibGfVZW_"}},{"cell_type":"markdown","source":["# Question 02: [ Marks 10 ]\n","\n","What is a decision boundary in a perceptron?\n","Explain how the equation\n","w₁x₁ + w₂x₂ + b = 0\n","represents a decision boundary geometrically."],"metadata":{"id":"_tPnXmkMVi8G"}},{"cell_type":"markdown","source":["## Write Answer 02:\n","\n","\n","A decision boundary in a perceptron is the line (or hyperplane) that separates different classes of data based on the perceptron’s output.\n","\n","Geometrically, the equation\n","w₁x₁ + w₂x₂ + b = 0\n","represents a straight line in a 2D space. This line divides the plane into two regions: on one side the expression is positive and the perceptron predicts one class, and on the other side it is negative and predicts the other class. The weights w₁ and w₂ control the slope and orientation of the line, while b shifts the line’s position.\n"],"metadata":{"id":"oTQ9yUrlVk4H"}},{"cell_type":"markdown","source":["# Question 03: [ Marks 15 ]\n","\n","Why is the perceptron called a linear classifier?\n","Explain your answer using the concept of linear separability and logical gate examples such as AND, OR, and XOR."],"metadata":{"id":"YiUWig4VVnXu"}},{"cell_type":"markdown","source":["## Write Answer 03:\n","A perceptron is called a linear classifier because it separates data using a straight line or, in higher dimensions, a linear hyperplane.\n","\n","It works only when the data is linearly separable, meaning a single straight line can divide the classes correctly. Logical gates like AND and OR are linearly separable. You can draw one straight line that separates their output classes, so a perceptron can learn them easily.However, XOR is not linearly separable. No single straight line can separate the output classes of XOR. Because a perceptron can form only one linear decision boundary, it fails to learn XOR. This limitation is why a single perceptron cannot solve non-linear problems and why multilayer neural networks are needed."],"metadata":{"id":"kTpWEibjaGmB"}},{"cell_type":"markdown","source":["# Question 04: [ Marks 15 ]\n","\n","Write the perceptron weight update rule and explain it intuitively.\n","Why are the weights updated only when the prediction is incorrect?"],"metadata":{"id":"Vh-CVlcIZMdT"}},{"cell_type":"markdown","source":["## Write Answer 04:\n","\n","Perceptron Weight Update Rule:\n","\n","w_i = w_i + n (y - ^y) x_i\n","b   = b + n (y - ^y)\n","\n","Where:\n","w_i = weight\n","x_i = input feature\n","y   = true label\n","^y   = predicted output\n","n   = learning rate\n","\n","If the perceptron makes a wrong prediction, the weights are adjusted in a direction that moves the decision boundary closer to correctly classifying that input. If the prediction is correct, no change is needed because the current weights already give the right output.\n","\n","\n","When the prediction is correct, changing the weights could move the decision boundary away from a good solution. Updating only on errors helps the perceptron converge to a decision boundary that correctly separates the data."],"metadata":{"id":"rfdsvUWfaJJY"}},{"cell_type":"markdown","source":["# Question 05: [ Marks 10 ]\n","\n","Why does a single-layer perceptron fail to solve the XOR problem?\n","What does this limitation tell us about the need for multilayer neural networks?"],"metadata":{"id":"WIr6YmxaZNli"}},{"cell_type":"markdown","source":["## Write Answer 05:\n","\n","A single-layer perceptron cannot solve the XOR problem because XOR is **not linearly separable**.\n","\n","- XOR truth table:\n","  - [0, 0] → 0\n","  - [0, 1] → 1\n","  - [1, 0] → 1\n","  - [1, 1] → 0\n","\n","No single straight line can separate the 1s from the 0s in 2D space.  \n","A single-layer perceptron can only form **one linear decision boundary**, so it cannot classify XOR correctly.\n","\n","**Implication:**  \n","Some problems require **non-linear decision boundaries**.  \n","This is why we need **multilayer neural networks (MLPs)**, which can combine multiple layers to solve complex, non-linear problems like XOR."],"metadata":{"id":"RVJMptexaKjg"}},{"cell_type":"markdown","source":["# Question 06: [ Marks 20 ]\n","\n","What is meant by the “perceptron Learning Rule” or weight adjustment method?\n","Why can adjusting one weight affect the classification of other points?"],"metadata":{"id":"t7wLPZTPZOnT"}},{"cell_type":"markdown","source":["The Perceptron Learning Rule is a method to adjust the weights and bias of a perceptron to reduce classification errors.\n","Weights are updated only when the perceptron makes a wrong prediction, moving the decision boundary so that the input is more likely to be classified correctly next time.\n","\n","Weight update formulas:\n","\n","w_i = w_i + n * (y - y_hat) * x_i\n","b   = b + n * (y - y_hat)\n","\n","Where:\n","w_i   = weight for input x_i\n","x_i   = input feature\n","y     = true label\n","y_hat = predicted output\n","n   = learning rate\n","b     = bias\n","\n","Why adjusting one weight can affect other points:\n","- The perceptron’s decision boundary depends on all weights and bias.\n","- Changing a weight moves the boundary in input space. This can correct one point but may misclassify others that were previously correct.\n","- Training iteratively over all points gradually adjusts the weights until all points are classified correctly (if the data is linearly separable)."],"metadata":{"id":"qNmHw4tnaMFZ"}},{"cell_type":"markdown","source":["# Question 07: [ Marks 20 ]\n","\n","What are the main limitations of a single-layer perceptron?\n","How does the idea of using more than one layer (MLP) help overcome these limitations at a high level accoriding to you?\n","\n"],"metadata":{"id":"zZMx7oNaZPqc"}},{"cell_type":"markdown","source":["## Write Answer 07:\n","**Limitations of a Single-Layer Perceptron**\n","\n","1. **Can only solve linearly separable problems**:  \n","   A single-layer perceptron creates only one linear decision boundary, so it cannot solve problems like XOR or any problem where classes are not separable by a straight line.\n","\n","2. **Limited representational power**:  \n","   It cannot capture complex patterns, relationships, or non-linear features in data.\n","\n","3. **Cannot approximate arbitrary functions**:  \n","   Because of its linear nature, it cannot model non-linear mappings from inputs to outputs.\n","\n","**How Multilayer Perceptrons (MLPs) overcome these limitations**\n","\n","- By stacking multiple layers of neurons, MLPs can create **non-linear decision boundaries**.  \n","- Each layer transforms the data, allowing the network to combine features in complex ways.  \n","- This enables MLPs to solve **non-linear problems**, approximate complex functions, and learn intricate patterns in data that a single-layer perceptron cannot handle.\n"],"metadata":{"id":"L3cixQ3gaN0J"}}]}