{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1mdhWoyoh-aBqIJ1GoxCQyAhZ_S3nNCd6","timestamp":1770020952214}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"E8yaVzW_tScT","executionInfo":{"status":"ok","timestamp":1770021019721,"user_tz":-360,"elapsed":2635,"user":{"displayName":"Rifat Rizvi","userId":"02592282572987649717"}}},"outputs":[],"source":["# ===============================\n","# PERCEPTRON CLASSIFICATION EXAMPLE\n","# ===============================\n","\n","import numpy as np\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import Perceptron\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# -------------------------------\n","# Load the dataset\n","# -------------------------------\n","iris = load_iris()\n","X = iris.data   # Features: Sepal length, Sepal width, Petal length, Petal width\n","y = iris.target # Labels: 0, 1, 2 (Setosa, Versicolor, Virginica)\n","\n"]},{"cell_type":"code","source":["y"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pOflz4q6tjuT","outputId":"5daeae75-9acf-49fd-c16a-dbce2e1cea8c","executionInfo":{"status":"ok","timestamp":1770021019734,"user_tz":-360,"elapsed":10,"user":{"displayName":"Rifat Rizvi","userId":"02592282572987649717"}}},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n","       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["# -------------------------------\n","# For simplicity, we do a binary classification:\n","# Let's classify Setosa (0) vs not-Setosa (1)\n","# -------------------------------\n","y_binary = np.where(y == 0, 0, 1) #np.where(condition, A, B)\n","\n","# -------------------------------\n","# Split into train and test\n","# -------------------------------\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y_binary, test_size=0.3, random_state=42\n",")\n","\n","# -------------------------------\n","# Scale the features\n","# -------------------------------\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","# -------------------------------\n","# Initialize and train Perceptron\n","# -------------------------------\n","perceptron = Perceptron(max_iter=10000, eta0=0.01, random_state=1)\n","perceptron.fit(X_train, y_train)\n","\n","# -------------------------------\n","# Make predictions\n","# -------------------------------\n","y_pred_train = perceptron.predict(X_train)\n","y_pred_test = perceptron.predict(X_test)\n","\n","# -------------------------------\n","# Evaluate the model\n","# -------------------------------\n","print(\"Train Accuracy:\", accuracy_score(y_train, y_pred_train))\n","print(\"Test Accuracy :\", accuracy_score(y_test, y_pred_test))\n","print(\"\\nClassification Report (Test):\\n\", classification_report(y_test, y_pred_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DKF89Cd5te65","outputId":"174ca8a5-241f-42a3-887b-34cedff71992","executionInfo":{"status":"ok","timestamp":1770021019824,"user_tz":-360,"elapsed":85,"user":{"displayName":"Rifat Rizvi","userId":"02592282572987649717"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Accuracy: 1.0\n","Test Accuracy : 1.0\n","\n","Classification Report (Test):\n","               precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        19\n","           1       1.00      1.00      1.00        26\n","\n","    accuracy                           1.00        45\n","   macro avg       1.00      1.00      1.00        45\n","weighted avg       1.00      1.00      1.00        45\n","\n"]}]}]}